{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neurokit2 as nk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import configparser\n",
    "import os\n",
    "import os.path as osp\n",
    "import pickle\n",
    "from eda_signal_processing import *\n",
    "from typing import Dict, List\n",
    "from tqdm import tqdm\n",
    "import scipy"
   ]
  },
  {
   "source": [
    "# Load eda signal from datasets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Initialize file paths"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_folder_path(dataset_name: str) -> str:\n",
    "    # Read dataset path from config.ini file\n",
    "    config_path = osp.join(osp.dirname(os.getcwd()), 'config.ini')\n",
    "    parser = configparser.ConfigParser()\n",
    "    parser.read(config_path)\n",
    "    dataset_folder_path = None\n",
    "    if dataset_name == 'AffectiveROAD':\n",
    "        dataset_folder_path = parser['DATA_PATH']['affectiveROAD_dataset_path']\n",
    "    elif dataset_name in ['WESAD_CHEST', 'WESAD_WRIST']:\n",
    "        dataset_folder_path = parser['DATA_PATH']['wesad_dataset_path']\n",
    "    elif dataset_name == 'DCU_NVT_EXP1':\n",
    "        dataset_folder_path = parser['DATA_PATH']['dcu_nvt_dataset_path']\n",
    "    return dataset_folder_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_raw_dataset(dataset_name: str):\n",
    "    dataset = None\n",
    "    # Initialize dataset folder paths\n",
    "    dataset_folder_path = get_dataset_folder_path(dataset_name)\n",
    "    if dataset_name == 'AffectiveROAD':\n",
    "        # Initialize dataset paths\n",
    "        affectiveROAD_dataset_file_path = osp.join(dataset_folder_path, 'affectiveROAD_dataset.pkl')\n",
    "        dataset = pickle.load(open(affectiveROAD_dataset_file_path, 'rb')) # Load affectiveROAD dataset -> sampling_rate = 4 Hz\n",
    "    elif dataset_name == 'WESAD_CHEST':\n",
    "        # Initialize dataset paths\n",
    "        wesad_chest_file_path = osp.join(dataset_folder_path, 'wesad_chest_dataset.pkl')\n",
    "        dataset = pickle.load(open(wesad_chest_file_path, 'rb')) # Load WESAD_CHEST dataset -> sampling_rate = 700 Hz\n",
    "    elif dataset_name == 'WESAD_WRIST':\n",
    "        # Initialize dataset paths\n",
    "        wesad_wrist_file_path = osp.join(dataset_folder_path, 'wesad_wrist_dataset.pkl')\n",
    "        dataset = pickle.load(open(wesad_wrist_file_path, 'rb')) # Load WESAD_WRIST dataset -> sampling_rate = 4 Hz\n",
    "    elif dataset_name == 'DCU_NVT_EXP1':\n",
    "        # Initialize dataset paths\n",
    "        dcu_nvt_file_path = osp.join(dataset_folder_path, 'DCU_NVT_EXP1_dataset.pkl')\n",
    "        dataset = pickle.load(open(dcu_nvt_file_path, 'rb')) # Load DCU_NVT_EXP1 dataset -> sampling_rate = 5 Hz\n",
    "    return dataset"
   ]
  },
  {
   "source": [
    "## Load datasets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Uncomment the dataset that you wanna load -- # \n",
    "# dataset_name = 'AffectiveROAD'\n",
    "# dataset_name = 'WESAD_CHEST'\n",
    "# dataset_name = 'WESAD_WRIST'\n",
    "dataset_name = 'DCU_NVT_EXP1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_raw_dataset(dataset_name) # Load dataset\n",
    "eda = dataset['eda'] # Get raw EDA signal\n",
    "ground_truth = dataset['ground_truth'] # Get its corresponding ground-truth"
   ]
  },
  {
   "source": [
    "# Extract statistical features"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Declare functions to process and extract statistical features"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract statistical features from the EDA signal with a current WINDOW_SIZE and WINDOW_SHIFT\n",
    "def extract_stats_features(eda: Dict[str, Dict[str, List[float]]], window_size: int, window_shift: int, sampling_rate: int) -> np.array:\n",
    "    \"\"\" This function extract stats feature corresponding to left-side of the current eda signal with length equals to window_size \"\"\" \n",
    "    # window_size: unit -> seconds - the length of signal which is cut to extract statistical feature equals to 60 seconds\n",
    "    # window_shift: unit -> seconds - the step of the sliding window\n",
    "    # sampling_rate: unit -> Hz - the number of recorded points per second\n",
    "    stats_features = []\n",
    "    for user_id, data in tqdm(eda.items()):\n",
    "        for task_id, eda_signal in data.items():\n",
    "            len_eda_signal = len(eda_signal)\n",
    "            step = window_shift * sampling_rate # The true step to slide along the time axis of the signal\n",
    "            first_iter = window_size * sampling_rate # The true index of the signal at a time-point \n",
    "            for current_iter in range(first_iter, len_eda_signal, step): # current_iter is \"second_iter\"\n",
    "                previous_iter = current_iter - first_iter\n",
    "                signal = eda_signal[previous_iter:current_iter]\n",
    "                eda_features = extract_eda_features(signal, sampling_rate) # Extract SCR, SCL, Onset, Offset, Peaks, etc.\n",
    "                eda_stats_features = extract_statistics_eda_features(eda_features) # Extract statistical features from extracted EDA features\n",
    "                stats_features.append(eda_stats_features)\n",
    "    stats_features = np.array(stats_features) # Transform to numpy array format\n",
    "    return stats_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_ground_truth(ground_truth: Dict[str, Dict[str, List[int]]], window_size: int, window_shift: int, sampling_rate: int) -> np.array: \n",
    "    \"\"\" This function should be call after the function extract_stats_features is called.\n",
    "        The iterative order of this function is the same as extract_stats_features function to maintain the integrity of the dataset.\n",
    "    \"\"\"\n",
    "    # window_size: unit -> seconds - the length of signal which is cut to extract statistical feature equals to 60 seconds\n",
    "    # window_shift: unit -> seconds - the step of the sliding window\n",
    "    # sampling_rate: unit -> Hz - the number of recorded points per second\n",
    "    gt = []\n",
    "    for user_id, data in tqdm(ground_truth.items()):\n",
    "        for task_id, _ground_truth in data.items():\n",
    "            len_ground_truth = len(_ground_truth)\n",
    "            start_index = window_size * sampling_rate # The true index of the signal at a time-point\n",
    "            step = window_shift * sampling_rate\n",
    "            gt += [_ground_truth[index] for index in range(start_index, len_ground_truth, step)] # Append the flatten array \n",
    "    gt = np.array(gt) # Transform to numpy array format\n",
    "    return gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_groups_from_ground_truth(ground_truth: Dict[str, Dict[str, List[int]]], window_size: int, window_shift: int, sampling_rate: int) -> np.array:\n",
    "    \"\"\" This function should be call after the function extract_stats_features is called.\n",
    "        The iterative order of this function is the same as extract_stats_features and map_ground_truth functions to maintain the integrity of the dataset.\n",
    "        Generate user_id label for each statistical features using ground-truth\n",
    "    \"\"\"\n",
    "    # window_size: unit -> seconds - the length of signal which is cut to extract statistical feature equals to 60 seconds\n",
    "    # window_shift: unit -> seconds - the step of the sliding window\n",
    "    # sampling_rate: unit -> Hz - the number of recorded points per second\n",
    "    groups = []\n",
    "    for user_id, data in tqdm(ground_truth.items()):\n",
    "        for task_id, _ground_truth in data.items():\n",
    "            len_ground_truth = len(_ground_truth)\n",
    "            start_index = window_size * sampling_rate\n",
    "            step = window_shift * sampling_rate\n",
    "            groups += [user_id for _ in range(start_index, len_ground_truth, step)]\n",
    "    groups = np.array(groups)\n",
    "    return groups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sampling_rate(dataset_name: str) -> int:\n",
    "    sampling_rate = None\n",
    "    if dataset_name in ['AffectiveROAD', 'WESAD_WRIST']:\n",
    "        sampling_rate = 4\n",
    "    elif dataset_name == 'WESAD_CHEST':\n",
    "        sampling_rate = 700\n",
    "    elif dataset_name == 'DCU_NVT_EXP1':\n",
    "        sampling_rate = 5\n",
    "    return sampling_rate"
   ]
  },
  {
   "source": [
    "## Extract statistical features"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 60 # the length of signal which is cut to extract statistical feature equals to 60 seconds\n",
    "WINDOW_SHIFT = 20 # the step of the sliding window \n",
    "SAMPLING_RATE = get_sampling_rate(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract EDA statistical features \n",
    "eda_stats_features = extract_stats_features(eda, WINDOW_SIZE, WINDOW_SHIFT, SAMPLING_RATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map ground-truth to the features \n",
    "mapped_ground_truth = map_ground_truth(ground_truth, WINDOW_SIZE, WINDOW_SHIFT, SAMPLING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label the group of the data also\n",
    "groups = generate_data_groups_from_ground_truth(ground_truth, WINDOW_SIZE, WINDOW_SHIFT, SAMPLING_RATE)"
   ]
  },
  {
   "source": [
    "## Save extracted features and their corresponding ground-truth"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder_path = get_dataset_folder_path(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the features to files in .npy format\n",
    "feat_output_file_path = osp.join(dataset_folder_path, f'{dataset_name}_stats_feats.npy')\n",
    "np.save(feat_output_file_path, eda_stats_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the ground-truth of the corresponding signal at its corresponding time-point\n",
    "gt_output_file_path = osp.join(dataset_folder_path, f'{dataset_name}_ground_truth.npy')\n",
    "np.save(gt_output_file_path, mapped_ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the user_id mapping of the statistical features\n",
    "groups_output_file_path = osp.join(dataset_folder_path, f'{dataset_name}_groups.npy')\n",
    "np.save(groups_output_file_path, groups) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}