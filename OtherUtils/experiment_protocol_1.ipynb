{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.6 64-bit",
   "display_name": "Python 3.7.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "69a7ec1f19fe8e6b4d60bff3a060a39e42b8b78906aa04442e53246e4bcaf9b9"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import os.path as osp\n",
    "from DatasetLoader.dataset_loader import DatasetLoader\n",
    "from DatasetLoader.signal_processing import *\n",
    "from sklearn.impute import SimpleImputer\n",
    "from collections import defaultdict\n",
    "from typing import Tuple\n",
    "from Classifiers.classifiers import Classifier\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_labels = ['mean_scl', 'std_scl', 'std_scr', 'corr', \n",
    "                'num_responses', 'sum_scr_response_duration', 'sum_scr_amplitude', 'area_of_response_curve',\n",
    "                'num_scr_peaks', 'mean_eda', 'std_eda', 'min_eda', 'max_eda', 'eda_dynamic_range',\n",
    "                'mean_scr', 'max_scr', 'min_scr', 'kurtosis_scr', 'skewness_scr', 'mean_first_grad', 'std_first_grad', 'mean_second_grad', 'std_second_grad', \n",
    "                'mean_peaks', 'max_peaks', 'min_peaks', 'std_peaks', 'mean_onsets', 'max_onsets', 'min_onsets', 'std_onsets',\n",
    "                'ALSC', 'INSC', 'APSC', 'RMSC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(flatten_agg_data: np.array, flatten_agg_ground_truth: np.array, flatten_agg_group: np.array = None, sampling_rate: int = 5) -> Tuple[ np.array, np.array, np.array ]:\n",
    "    # Remove the cut with less than 50 values\n",
    "    # filtered_agg_index = [item_index for item_index, microsiemens in enumerate(flatten_agg_data) if len(microsiemens) >= 250]\n",
    "    filtered_agg_index = [item_index for item_index, microsiemens in enumerate(flatten_agg_data) if len(microsiemens) >= 150]\n",
    "    flatten_agg_data = flatten_agg_data[filtered_agg_index]\n",
    "    flatten_agg_ground_truth = flatten_agg_ground_truth[filtered_agg_index]\n",
    "    if flatten_agg_group is not None:\n",
    "        flatten_agg_group = flatten_agg_group[filtered_agg_index]\n",
    "\n",
    "    # Fill the missing values by constant = 0 (due to device error during data collection process)\n",
    "    imp_constant = SimpleImputer(strategy='constant', fill_value=0)\n",
    "    imputed_agg_data = np.array([imp_constant.fit_transform(microsiemens.reshape(-1, 1)).flatten() for microsiemens in flatten_agg_data])\n",
    "\n",
    "    return imputed_agg_data, flatten_agg_ground_truth, flatten_agg_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(preprocessed_data: np.array, sampling_rate: int = 5) -> np.array:\n",
    "    # Extract features from the data\n",
    "    # print([len(microsiemens) for microsiemens in preprocessed_data])\n",
    "    processed_features = [extract_gsr_features(microsiemens, sampling_rate=sampling_rate) for microsiemens in preprocessed_data]\n",
    "\n",
    "    # Extract statistic features\n",
    "    statistic_features = np.array([statistics_gsr_signal_features(feat) for feat in processed_features])\n",
    "    return statistic_features"
   ]
  },
  {
   "source": [
    "# Collected GSR Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_loader = DatasetLoader()\n",
    "collected_gsr_data, ground_truth = dataset_loader.load_collected_gsr_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_signal_data = select_single_signal(collected_gsr_data, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "interval_gsr_data, interval_ground_truth, interval_group = dataset_loader.divide_into_intervals(single_signal_data, ground_truth, 60, sampling_rate = 5)"
   ]
  },
  {
   "source": [
    "### One-minute interval split\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of subjects: 11\nNumber of samples: 640\n"
     ]
    }
   ],
   "source": [
    "flatten_agg_data = dataset_loader.flatten(interval_gsr_data)\n",
    "flatten_agg_ground_truth = dataset_loader.flatten(interval_ground_truth)\n",
    "flatten_agg_group = dataset_loader.flatten(interval_group)\n",
    "print(f\"Number of subjects: {len(list(set(flatten_agg_group)))}\")\n",
    "print(f\"Number of samples: {len(flatten_agg_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data, preprocessed_ground_truth, preprocessed_groups = preprocess_data(flatten_agg_data, flatten_agg_ground_truth, flatten_agg_group=flatten_agg_group)\n",
    "statistic_features = extract_features(preprocessed_data)"
   ]
  },
  {
   "source": [
    "## General Cross-population Classifier"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Binary Classification"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_ground_truth = np.array([0 if value[0] < 2 else 1 for value in preprocessed_ground_truth])\n",
    "X = statistic_features\n",
    "y = binary_ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0: 457\n1: 140\n"
     ]
    }
   ],
   "source": [
    "clf = Classifier(X, y, groups = preprocessed_groups, logo_validation = True, feature_labels = feature_labels)\n",
    "# test_groups, f1_scores = clf.logistic_regression()\n",
    "# test_groups, f1_scores = clf.random_forest_classifier()\n",
    "# test_groups, f1_scores = clf.support_vector_machine()\n",
    "# test_groups, f1_scores = clf.multilayer_perceptron()\n",
    "# test_groups, f1_scores = clf.knn_classifier()\n",
    "dataset_loader.class_percentage_analysis(y)\n",
    "# clf.dump_csv(test_groups, f1_scores, 'DCU-NVT-EXP1-General-KNN.csv')"
   ]
  },
  {
   "source": [
    "## Person-specific stress detection model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Binary classification"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare personal-specific stress detection model\n",
    "person_specific_dataset = defaultdict(dict)\n",
    "person_specific_ground_truth = defaultdict(dict)\n",
    "for participant_id, data in single_signal_data.items():\n",
    "    person_specific_data, person_specific_gt = dataset_loader.prepare_person_specific_dataset(data, ground_truth)\n",
    "    person_specific_dataset[participant_id] = person_specific_data\n",
    "    person_specific_ground_truth[participant_id] = person_specific_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "---- Participant id A ----\n",
      "[0.61666667 0.38333333] 0.6216216216216216\n",
      "Balanced\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "Best cv score: 0.7993265993265993\n",
      "{'n_neighbors': 7, 'weights': 'distance'}\n",
      "F1 Score: 0.5882352941176471\n",
      "Precision Score: 0.625\n",
      "Recall Score: 0.5555555555555556\n",
      "---------------------------\n",
      "---- Participant id B ----\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  18 | elapsed:    0.0s finished\n",
      "[0.69090909 0.30909091] 0.4473684210526316\n",
      "Imbalanced\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "Best cv score: 0.6746031746031745\n",
      "{'n_neighbors': 5, 'weights': 'uniform'}\n",
      "F1 Score: 0.588235294117647\n",
      "Precision Score: 0.5\n",
      "Recall Score: 0.7142857142857143\n",
      "---------------------------\n",
      "---- Participant id C ----\n",
      "---- Participant id D ----\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  18 | elapsed:    0.0s finished\n",
      "[0.40625 0.59375] 0.6842105263157895\n",
      "Balanced\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "Best cv score: 0.8408730158730159\n",
      "{'n_neighbors': 7, 'weights': 'uniform'}\n",
      "F1 Score: 0.9032258064516129\n",
      "Precision Score: 0.875\n",
      "Recall Score: 0.9333333333333333\n",
      "---------------------------\n",
      "---- Participant id E ----\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  18 | elapsed:    0.0s finished\n",
      "[0.60784314 0.39215686] 0.6451612903225807\n",
      "Balanced\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "Best cv score: 0.9333333333333332\n",
      "{'n_neighbors': 3, 'weights': 'uniform'}\n",
      "F1 Score: 0.4615384615384615\n",
      "Precision Score: 0.6\n",
      "Recall Score: 0.375\n",
      "---------------------------\n",
      "---- Participant id F ----\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  18 | elapsed:    0.0s finished\n",
      "[0.81355932 0.18644068] 0.22916666666666666\n",
      "Imbalanced\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "Best cv score: 0.4444444444444444\n",
      "{'n_neighbors': 7, 'weights': 'uniform'}\n",
      "F1 Score: 0.0\n",
      "Precision Score: 0.0\n",
      "Recall Score: 0.0\n",
      "---------------------------\n",
      "---- Participant id G ----\n",
      "---- Participant id H ----\n",
      "---- Participant id I ----\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  18 | elapsed:    0.0s finished\n",
      "[0.84615385 0.15384615] 0.18181818181818182\n",
      "Imbalanced\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "Best cv score: 0.0\n",
      "{'n_neighbors': 3, 'weights': 'uniform'}\n",
      "F1 Score: 0.0\n",
      "Precision Score: 0.0\n",
      "Recall Score: 0.0\n",
      "---------------------------\n",
      "---- Participant id J ----\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  18 | elapsed:    0.0s finished\n",
      "[0.57407407 0.42592593] 0.7419354838709677\n",
      "Balanced\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "Best cv score: 0.6051282051282051\n",
      "{'n_neighbors': 3, 'weights': 'distance'}\n",
      "F1 Score: 0.5882352941176471\n",
      "Precision Score: 0.625\n",
      "Recall Score: 0.5555555555555556\n",
      "---------------------------\n",
      "---- Participant id K ----\n",
      "------------------------------------\n",
      "Mean F1-score: 0.44706716433471655\n",
      "Mean precision score: 0.46071428571428574\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  18 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "f1_scores = []\n",
    "precision_scores = []\n",
    "test_groups = []\n",
    "for participant_id, data in person_specific_dataset.items():\n",
    "    print(f\"---- Participant id {participant_id} ----\")\n",
    "    person_specific_gt = np.array([0 if value[0] < 2 else 1 for value in person_specific_ground_truth[participant_id]])\n",
    "    if len(list(set(person_specific_gt))) < 2: # If the data contains non-stress value, then continue\n",
    "        continue\n",
    "    test_groups.append(participant_id)\n",
    "    person_specific_data, person_specific_gt = dataset_loader.divide_person_specific_data_into_intervals(data, person_specific_gt, num_samples = 60)\n",
    "    preprocessed_data, preprocessed_ground_truth, _ = preprocess_data(person_specific_data, person_specific_gt)\n",
    "    person_specific_statistic_features = extract_features(preprocessed_data)\n",
    "    total_classes = len(preprocessed_ground_truth)\n",
    "    class_percentage = np.array([num_items / total_classes for _, num_items in Counter(preprocessed_ground_truth).items()])\n",
    "    balanced_degree_score = 1.0 * class_percentage.min() / class_percentage.max()\n",
    "    print(class_percentage, balanced_degree_score)\n",
    "    if balanced_degree_score < 0.5:\n",
    "        print(\"Imbalanced\")\n",
    "        # clf = Classifier(person_specific_statistic_features, preprocessed_ground_truth, balanced_weight = 'balanced', cross_validation = True, feature_labels = feature_labels)\n",
    "    else:\n",
    "        print(\"Balanced\")\n",
    "    clf = Classifier(person_specific_statistic_features, preprocessed_ground_truth, cross_validation = True, feature_labels = feature_labels)\n",
    "    # f1score, precision = clf.logistic_regression()\n",
    "    # f1score, precision = clf.random_forest_classifier()\n",
    "    # f1score, precision = clf.support_vector_machine()\n",
    "    # f1score, precision = clf.multilayer_perceptron()\n",
    "    f1score, precision = clf.knn_classifier()\n",
    "    f1_scores.append(f1score)\n",
    "    precision_scores.append(precision)\n",
    "    print(\"---------------------------\")\n",
    "print(\"------------------------------------\")\n",
    "clf.dump_csv(test_groups, f1_scores, 'DCU-NVT-EXP1-Personal-KNN.csv')\n",
    "mean_f1_score = np.mean(f1_scores)\n",
    "print(f\"Mean F1-score: {mean_f1_score}\")\n",
    "mean_precision_score = np.mean(precision_scores)\n",
    "print(f\"Mean precision score: {mean_precision_score}\")"
   ]
  },
  {
   "source": [
    "# WESAD GSR Data Low-Sample Simulated"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_loader = DatasetLoader()\n",
    "wesad_gsr_data, ground_truth = dataset_loader.load_wesad_gsr_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_signal_data = select_single_signal(wesad_gsr_data, 0)\n",
    "downsampling_wesad_gsr_data = resampling_data_signal(single_signal_data, sampling_rate = 700, desired_sampling_rate = 5, method = 'interpolation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "interval_gsr_data, interval_ground_truth, interval_group = dataset_loader.divide_into_intervals(downsampling_wesad_gsr_data, ground_truth, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of subjects: 15\nNumber of samples: 75\n"
     ]
    }
   ],
   "source": [
    "flatten_agg_data = dataset_loader.flatten(interval_gsr_data)\n",
    "flatten_agg_ground_truth = dataset_loader.flatten(interval_ground_truth)\n",
    "flatten_agg_group = dataset_loader.flatten(interval_group)\n",
    "print(f\"Number of subjects: {len(list(set(flatten_agg_group)))}\")\n",
    "print(f\"Number of samples: {len(flatten_agg_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data, preprocessed_ground_truth, preprocessed_groups = preprocess_data(flatten_agg_data, flatten_agg_ground_truth, flatten_agg_group=flatten_agg_group)\n",
    "statistic_features = extract_features(preprocessed_data)"
   ]
  },
  {
   "source": [
    "## General Cross-population Classifier"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Binary classification"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_ground_truth = preprocessed_ground_truth.flatten()\n",
    "X = statistic_features\n",
    "y = binary_ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Classifier(X, y, groups = preprocessed_groups, logo_validation = True, feature_labels = feature_labels)\n",
    "# test_groups, f1_scores = clf.logistic_regression()\n",
    "# test_groups, f1_scores = clf.random_forest_classifier()\n",
    "# test_groups, f1_scores = clf.support_vector_machine()\n",
    "# test_groups, f1_scores = clf.multilayer_perceptron()\n",
    "# test_groups, f1_scores = clf.knn_classifier()\n",
    "# clf.dump_csv(test_groups, f1_scores, 'WESAD-General-RF.csv')"
   ]
  },
  {
   "source": [
    "### Person-specific stress detection"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare personal-specific stress detection model\n",
    "person_specific_dataset = defaultdict(dict)\n",
    "person_specific_ground_truth = defaultdict(dict)\n",
    "for participant_id, data in downsampling_wesad_gsr_data.items():\n",
    "    person_specific_data, person_specific_gt = dataset_loader.prepare_person_specific_dataset(data, ground_truth)\n",
    "    person_specific_dataset[participant_id] = person_specific_data\n",
    "    person_specific_ground_truth[participant_id] = person_specific_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "---- Participant id S10 ----\n",
      "[0.75510204 0.24489796] 0.32432432432432434\n",
      "Imbalanced\n",
      "Fitting 3 folds for each of 144 candidates, totalling 432 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    4.1s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   16.5s\n",
      "[Parallel(n_jobs=-1)]: Done 432 out of 432 | elapsed:   37.5s finished\n",
      "Best cv score: 0.9333333333333332\n",
      "{'class_weight': None, 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "F1 Score: 0.888888888888889\n",
      "Precision Score: 1.0\n",
      "Recall Score: 0.8\n",
      "---------------------------\n",
      "---- Participant id S11 ----\n",
      "[0.77083333 0.22916667] 0.29729729729729726\n",
      "Imbalanced\n",
      "Fitting 3 folds for each of 144 candidates, totalling 432 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   13.9s\n",
      "[Parallel(n_jobs=-1)]: Done 432 out of 432 | elapsed:   34.3s finished\n",
      "Best cv score: 0.6666666666666666\n",
      "{'class_weight': None, 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "F1 Score: 0.7499999999999999\n",
      "Precision Score: 1.0\n",
      "Recall Score: 0.6\n",
      "---------------------------\n",
      "---- Participant id S13 ----\n",
      "[0.77083333 0.22916667] 0.29729729729729726\n",
      "Imbalanced\n",
      "Fitting 3 folds for each of 144 candidates, totalling 432 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   13.9s\n",
      "[Parallel(n_jobs=-1)]: Done 432 out of 432 | elapsed:   33.9s finished\n",
      "Best cv score: 1.0\n",
      "{'class_weight': None, 'max_depth': 32, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 100}\n",
      "F1 Score: 0.888888888888889\n",
      "Precision Score: 1.0\n",
      "Recall Score: 0.8\n",
      "---------------------------\n",
      "---- Participant id S14 ----\n",
      "[0.77083333 0.22916667] 0.29729729729729726\n",
      "Imbalanced\n",
      "Fitting 3 folds for each of 144 candidates, totalling 432 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   13.4s\n",
      "[Parallel(n_jobs=-1)]: Done 432 out of 432 | elapsed:   32.9s finished\n",
      "Best cv score: 0.6666666666666666\n",
      "{'class_weight': 'balanced_subsample', 'max_depth': 55, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "F1 Score: 0.28571428571428575\n",
      "Precision Score: 0.5\n",
      "Recall Score: 0.2\n",
      "---------------------------\n",
      "---- Participant id S15 ----\n",
      "[0.77083333 0.22916667] 0.29729729729729726\n",
      "Imbalanced\n",
      "Fitting 3 folds for each of 144 candidates, totalling 432 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   13.0s\n",
      "[Parallel(n_jobs=-1)]: Done 432 out of 432 | elapsed:   32.8s finished\n",
      "Best cv score: 0.6666666666666666\n",
      "{'class_weight': 'balanced', 'max_depth': 32, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 100}\n",
      "F1 Score: 0.33333333333333337\n",
      "Precision Score: 1.0\n",
      "Recall Score: 0.2\n",
      "---------------------------\n",
      "---- Participant id S16 ----\n",
      "[0.77083333 0.22916667] 0.29729729729729726\n",
      "Imbalanced\n",
      "Fitting 3 folds for each of 144 candidates, totalling 432 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   13.1s\n",
      "[Parallel(n_jobs=-1)]: Done 432 out of 432 | elapsed:   32.6s finished\n",
      "Best cv score: 0.9333333333333332\n",
      "{'class_weight': None, 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100}\n",
      "F1 Score: 0.888888888888889\n",
      "Precision Score: 1.0\n",
      "Recall Score: 0.8\n",
      "---------------------------\n",
      "---- Participant id S17 ----\n",
      "[0.75 0.25] 0.3333333333333333\n",
      "Imbalanced\n",
      "Fitting 3 folds for each of 144 candidates, totalling 432 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   13.6s\n",
      "[Parallel(n_jobs=-1)]: Done 432 out of 432 | elapsed:   33.1s finished\n",
      "Best cv score: 0.9523809523809524\n",
      "{'class_weight': None, 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100}\n",
      "F1 Score: 0.5714285714285715\n",
      "Precision Score: 1.0\n",
      "Recall Score: 0.4\n",
      "---------------------------\n",
      "---- Participant id S2 ----\n",
      "[0.78723404 0.21276596] 0.2702702702702703\n",
      "Imbalanced\n",
      "Fitting 3 folds for each of 144 candidates, totalling 432 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   14.6s\n",
      "[Parallel(n_jobs=-1)]: Done 432 out of 432 | elapsed:   35.5s finished\n",
      "Best cv score: 0.8333333333333334\n",
      "{'class_weight': None, 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "F1 Score: 0.6666666666666666\n",
      "Precision Score: 1.0\n",
      "Recall Score: 0.5\n",
      "---------------------------\n",
      "---- Participant id S3 ----\n",
      "[0.78723404 0.21276596] 0.2702702702702703\n",
      "Imbalanced\n",
      "Fitting 3 folds for each of 144 candidates, totalling 432 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   13.3s\n",
      "[Parallel(n_jobs=-1)]: Done 432 out of 432 | elapsed:   32.7s finished\n",
      "Best cv score: 0.8888888888888888\n",
      "{'class_weight': None, 'max_depth': 55, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "F1 Score: 0.6666666666666665\n",
      "Precision Score: 0.6\n",
      "Recall Score: 0.75\n",
      "---------------------------\n",
      "---- Participant id S4 ----\n",
      "[0.78723404 0.21276596] 0.2702702702702703\n",
      "Imbalanced\n",
      "Fitting 3 folds for each of 144 candidates, totalling 432 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   13.2s\n",
      "[Parallel(n_jobs=-1)]: Done 432 out of 432 | elapsed:   32.7s finished\n",
      "Best cv score: 1.0\n",
      "{'class_weight': None, 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "F1 Score: 0.8571428571428571\n",
      "Precision Score: 1.0\n",
      "Recall Score: 0.75\n",
      "---------------------------\n",
      "---- Participant id S5 ----\n",
      "[0.79166667 0.20833333] 0.26315789473684215\n",
      "Imbalanced\n",
      "Fitting 3 folds for each of 144 candidates, totalling 432 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   13.3s\n",
      "[Parallel(n_jobs=-1)]: Done 432 out of 432 | elapsed:   32.5s finished\n",
      "Best cv score: 1.0\n",
      "{'class_weight': None, 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "F1 Score: 1.0\n",
      "Precision Score: 1.0\n",
      "Recall Score: 1.0\n",
      "---------------------------\n",
      "---- Participant id S6 ----\n",
      "[0.77083333 0.22916667] 0.29729729729729726\n",
      "Imbalanced\n",
      "Fitting 3 folds for each of 144 candidates, totalling 432 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   13.3s\n",
      "[Parallel(n_jobs=-1)]: Done 432 out of 432 | elapsed:   33.2s finished\n",
      "Best cv score: 0.8222222222222223\n",
      "{'class_weight': None, 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "F1 Score: 0.888888888888889\n",
      "Precision Score: 1.0\n",
      "Recall Score: 0.8\n",
      "---------------------------\n",
      "---- Participant id S7 ----\n",
      "[0.78723404 0.21276596] 0.2702702702702703\n",
      "Imbalanced\n",
      "Fitting 3 folds for each of 144 candidates, totalling 432 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   13.1s\n",
      "[Parallel(n_jobs=-1)]: Done 432 out of 432 | elapsed:   32.5s finished\n",
      "Best cv score: 1.0\n",
      "{'class_weight': None, 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "F1 Score: 1.0\n",
      "Precision Score: 1.0\n",
      "Recall Score: 1.0\n",
      "---------------------------\n",
      "---- Participant id S8 ----\n",
      "[0.77083333 0.22916667] 0.29729729729729726\n",
      "Imbalanced\n",
      "Fitting 3 folds for each of 144 candidates, totalling 432 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   13.5s\n",
      "[Parallel(n_jobs=-1)]: Done 432 out of 432 | elapsed:   33.0s finished\n",
      "Best cv score: 0.7777777777777777\n",
      "{'class_weight': None, 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "F1 Score: 0.5\n",
      "Precision Score: 0.6666666666666666\n",
      "Recall Score: 0.4\n",
      "---------------------------\n",
      "---- Participant id S9 ----\n",
      "[0.78723404 0.21276596] 0.2702702702702703\n",
      "Imbalanced\n",
      "Fitting 3 folds for each of 144 candidates, totalling 432 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   13.3s\n",
      "Best cv score: 0.9333333333333332\n",
      "{'class_weight': None, 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "F1 Score: 0.75\n",
      "Precision Score: 0.75\n",
      "Recall Score: 0.75\n",
      "---------------------------\n",
      "------------------------------------\n",
      "Mean F1-score: 0.729100529100529\n",
      "Mean precision score: 0.9011111111111111\n",
      "[Parallel(n_jobs=-1)]: Done 432 out of 432 | elapsed:   32.4s finished\n"
     ]
    }
   ],
   "source": [
    "f1_scores = []\n",
    "precision_scores = []\n",
    "test_groups = []\n",
    "for participant_id, data in person_specific_dataset.items():\n",
    "    print(f\"---- Participant id {participant_id} ----\")\n",
    "    person_specific_gt = person_specific_ground_truth[participant_id].flatten()\n",
    "    if len(list(set(person_specific_gt))) < 2: # If the data contains non-stress value, then continue\n",
    "        continue\n",
    "    test_groups.append(participant_id)\n",
    "    person_specific_data, person_specific_gt = dataset_loader.divide_person_specific_data_into_intervals(data, person_specific_gt, num_samples=60)\n",
    "    preprocessed_data, preprocessed_ground_truth, _ = preprocess_data(person_specific_data, person_specific_gt)\n",
    "\n",
    "    total_classes = len(preprocessed_ground_truth)\n",
    "    class_percentage = np.array([num_items / total_classes for _, num_items in Counter(preprocessed_ground_truth).items()])\n",
    "    balanced_degree_score = 1.0 * class_percentage.min() / class_percentage.max()\n",
    "    print(class_percentage, balanced_degree_score)\n",
    "    \n",
    "    person_specific_statistic_features = extract_features(preprocessed_data)\n",
    "    if balanced_degree_score < 0.5:\n",
    "        print(\"Imbalanced\")\n",
    "        # clf = Classifier(person_specific_statistic_features, preprocessed_ground_truth, balanced_weight = 'balanced', cross_validation = True, feature_labels = feature_labels)\n",
    "    else:\n",
    "        print(\"Balanced\")\n",
    "    clf = Classifier(person_specific_statistic_features, preprocessed_ground_truth, cross_validation = True, feature_labels = feature_labels)\n",
    "    # f1score, precision = clf.logistic_regression()\n",
    "    f1score, precision = clf.random_forest_classifier()\n",
    "    # f1score, precision = clf.decision_tree()\n",
    "    # f1score, precision = clf.support_vector_machine()\n",
    "    # f1score, precision = clf.multilayer_perceptron()\n",
    "    # f1score, precision = clf.knn_classifier()\n",
    "    # f1score, precision = clf.extra_tree_classifier()\n",
    "    # f1score, precision = clf.lda_classifier()\n",
    "    # f1score, precision = clf.logistic_regression_recursive_feature_selection()\n",
    "    f1_scores.append(f1score)\n",
    "    precision_scores.append(precision)\n",
    "    print(\"---------------------------\")\n",
    "print(\"------------------------------------\")\n",
    "clf.dump_csv(test_groups, f1_scores, 'WESAD-Personal-RF.csv')\n",
    "mean_f1_score = np.mean(f1_scores)\n",
    "print(f\"Mean F1-score: {mean_f1_score}\")\n",
    "mean_precision_score = np.mean(precision_scores)\n",
    "print(f\"Mean precision score: {mean_precision_score}\")"
   ]
  },
  {
   "source": [
    "# WESAD GSR Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_loader = DatasetLoader()\n",
    "wesad_gsr_data, ground_truth = dataset_loader.load_wesad_gsr_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_signal_data = select_single_signal(wesad_gsr_data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interval_gsr_data, interval_ground_truth, interval_group = dataset_loader.divide_into_intervals(single_signal_data, ground_truth, 60, sampling_rate = 700)\n",
    "interval_gsr_data, interval_ground_truth, interval_group = dataset_loader.divide_into_intervals(single_signal_data, ground_truth, 60, sampling_rate = 4)"
   ]
  },
  {
   "source": [
    "### One-minute interval split"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of subjects: 15\nNumber of samples: 788\n"
     ]
    }
   ],
   "source": [
    "flatten_agg_data = dataset_loader.flatten(interval_gsr_data)\n",
    "flatten_agg_ground_truth = dataset_loader.flatten(interval_ground_truth)\n",
    "flatten_agg_group = dataset_loader.flatten(interval_group)\n",
    "print(f\"Number of subjects: {len(list(set(flatten_agg_group)))}\")\n",
    "print(f\"Number of samples: {len(flatten_agg_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessed_ground_truth = flatten_agg_ground_truth\n",
    "# preprocessed_groups = flatten_agg_group\n",
    "# statistic_features = extract_features(flatten_agg_data, sampling_rate = 700)\n",
    "preprocessed_data, preprocessed_ground_truth, preprocessed_groups = preprocess_data(flatten_agg_data, flatten_agg_ground_truth, flatten_agg_group=flatten_agg_group)\n",
    "statistic_features = extract_features(preprocessed_data, sampling_rate = 4)"
   ]
  },
  {
   "source": [
    "## General Cross-population Classifier"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Binary Classification"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_ground_truth = preprocessed_ground_truth.flatten()\n",
    "X = statistic_features\n",
    "y = binary_ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0: 571\n1: 165\n"
     ]
    }
   ],
   "source": [
    "clf = Classifier(X, y, groups = preprocessed_groups, logo_validation = True, feature_labels = feature_labels)\n",
    "# test_groups, f1_scores = clf.logistic_regression()\n",
    "# test_groups, f1_scores = clf.random_forest_classifier()\n",
    "# test_groups, f1_scores = clf.support_vector_machine()\n",
    "# test_groups, f1_scores = clf.multilayer_perceptron()\n",
    "# test_groups, f1_scores = clf.knn_classifier()\n",
    "dataset_loader.class_percentage_analysis(y)\n",
    "# clf.dump_csv(test_groups, f1_scores, 'WESAD-General-RF.csv')"
   ]
  },
  {
   "source": [
    "### Person-specific stress detection"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare personal-specific stress detection model\n",
    "person_specific_dataset = defaultdict(dict)\n",
    "person_specific_ground_truth = defaultdict(dict)\n",
    "for participant_id, data in single_signal_data.items():\n",
    "    person_specific_data, person_specific_gt = dataset_loader.prepare_person_specific_dataset(data, ground_truth)\n",
    "    person_specific_dataset[participant_id] = person_specific_data\n",
    "    person_specific_ground_truth[participant_id] = person_specific_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "---- Participant id S10 ----\n",
      "[0.76923077 0.23076923] 0.3\n",
      "Imbalanced\n",
      "Fitting 3 folds for each of 144 candidates, totalling 432 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   13.6s\n",
      "[Parallel(n_jobs=-1)]: Done 432 out of 432 | elapsed:   33.4s finished\n",
      "Best cv score: 1.0\n",
      "{'class_weight': None, 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100}\n",
      "F1 Score: 0.8000000000000002\n",
      "Precision Score: 0.8\n",
      "Recall Score: 0.8\n",
      "---------------------------\n",
      "---- Participant id S11 ----\n",
      "[0.7755102 0.2244898] 0.2894736842105263\n",
      "Imbalanced\n",
      "Fitting 3 folds for each of 144 candidates, totalling 432 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   13.0s\n",
      "[Parallel(n_jobs=-1)]: Done 432 out of 432 | elapsed:   35.3s finished\n",
      "Best cv score: 0.8222222222222223\n",
      "{'class_weight': None, 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "F1 Score: 0.75\n",
      "Precision Score: 0.75\n",
      "Recall Score: 0.75\n",
      "---------------------------\n",
      "---- Participant id S13 ----\n",
      "[0.78 0.22] 0.28205128205128205\n",
      "Imbalanced\n",
      "Fitting 3 folds for each of 144 candidates, totalling 432 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   13.8s\n",
      "[Parallel(n_jobs=-1)]: Done 432 out of 432 | elapsed:   33.4s finished\n",
      "Best cv score: 0.8888888888888888\n",
      "{'class_weight': None, 'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "F1 Score: 1.0\n",
      "Precision Score: 1.0\n",
      "Recall Score: 1.0\n",
      "---------------------------\n",
      "---- Participant id S14 ----\n",
      "[0.7755102 0.2244898] 0.2894736842105263\n",
      "Imbalanced\n",
      "Fitting 3 folds for each of 144 candidates, totalling 432 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   14.8s\n",
      "[Parallel(n_jobs=-1)]: Done 432 out of 432 | elapsed:   33.2s finished\n",
      "Best cv score: 0.7777777777777777\n",
      "{'class_weight': None, 'max_depth': 55, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "F1 Score: 0.6666666666666666\n",
      "Precision Score: 1.0\n",
      "Recall Score: 0.5\n",
      "---------------------------\n",
      "---- Participant id S15 ----\n",
      "[0.77083333 0.22916667] 0.29729729729729726\n",
      "Imbalanced\n",
      "Fitting 3 folds for each of 144 candidates, totalling 432 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   12.5s\n",
      "[Parallel(n_jobs=-1)]: Done 432 out of 432 | elapsed:   34.5s finished\n",
      "Best cv score: 1.0\n",
      "{'class_weight': None, 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "F1 Score: 0.888888888888889\n",
      "Precision Score: 1.0\n",
      "Recall Score: 0.8\n",
      "---------------------------\n",
      "---- Participant id S16 ----\n",
      "[0.7755102 0.2244898] 0.2894736842105263\n",
      "Imbalanced\n",
      "Fitting 3 folds for each of 144 candidates, totalling 432 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    3.9s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   18.7s\n",
      "[Parallel(n_jobs=-1)]: Done 432 out of 432 | elapsed:   43.5s finished\n",
      "Best cv score: 1.0\n",
      "{'class_weight': None, 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "F1 Score: 1.0\n",
      "Precision Score: 1.0\n",
      "Recall Score: 1.0\n",
      "---------------------------\n",
      "---- Participant id S17 ----\n",
      "[0.75510204 0.24489796] 0.32432432432432434\n",
      "Imbalanced\n",
      "Fitting 3 folds for each of 144 candidates, totalling 432 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   15.9s\n",
      "[Parallel(n_jobs=-1)]: Done 432 out of 432 | elapsed:   41.3s finished\n",
      "Best cv score: 0.8222222222222223\n",
      "{'class_weight': 'balanced', 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100}\n",
      "F1 Score: 0.6666666666666666\n",
      "Precision Score: 0.5714285714285714\n",
      "Recall Score: 0.8\n",
      "---------------------------\n",
      "---- Participant id S2 ----\n",
      "[0.78723404 0.21276596] 0.2702702702702703\n",
      "Imbalanced\n",
      "Fitting 3 folds for each of 144 candidates, totalling 432 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   16.1s\n",
      "[Parallel(n_jobs=-1)]: Done 432 out of 432 | elapsed:   36.9s finished\n",
      "Best cv score: 0.8222222222222223\n",
      "{'class_weight': None, 'max_depth': 32, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "F1 Score: 0.3333333333333333\n",
      "Precision Score: 0.5\n",
      "Recall Score: 0.25\n",
      "---------------------------\n",
      "---- Participant id S3 ----\n",
      "[0.7755102 0.2244898] 0.2894736842105263\n",
      "Imbalanced\n",
      "Fitting 3 folds for each of 144 candidates, totalling 432 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   14.6s\n",
      "[Parallel(n_jobs=-1)]: Done 432 out of 432 | elapsed:   38.3s finished\n",
      "Best cv score: 0.8222222222222223\n",
      "{'class_weight': 'balanced', 'max_depth': 55, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100}\n",
      "F1 Score: 0.4444444444444445\n",
      "Precision Score: 0.4\n",
      "Recall Score: 0.5\n",
      "---------------------------\n",
      "---- Participant id S4 ----\n",
      "[0.79166667 0.20833333] 0.26315789473684215\n",
      "Imbalanced\n",
      "Fitting 3 folds for each of 144 candidates, totalling 432 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   14.4s\n",
      "[Parallel(n_jobs=-1)]: Done 432 out of 432 | elapsed:   37.2s finished\n",
      "Best cv score: 1.0\n",
      "{'class_weight': None, 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "F1 Score: 1.0\n",
      "Precision Score: 1.0\n",
      "Recall Score: 1.0\n",
      "---------------------------\n",
      "---- Participant id S5 ----\n",
      "[0.7755102 0.2244898] 0.2894736842105263\n",
      "Imbalanced\n",
      "Fitting 3 folds for each of 144 candidates, totalling 432 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   15.4s\n",
      "[Parallel(n_jobs=-1)]: Done 432 out of 432 | elapsed:   35.6s finished\n",
      "Best cv score: 1.0\n",
      "{'class_weight': None, 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "F1 Score: 0.8\n",
      "Precision Score: 0.6666666666666666\n",
      "Recall Score: 1.0\n",
      "---------------------------\n",
      "---- Participant id S6 ----\n",
      "[0.7755102 0.2244898] 0.2894736842105263\n",
      "Imbalanced\n",
      "Fitting 3 folds for each of 144 candidates, totalling 432 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   14.0s\n",
      "[Parallel(n_jobs=-1)]: Done 432 out of 432 | elapsed:   35.6s finished\n",
      "Best cv score: 0.9333333333333332\n",
      "{'class_weight': None, 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "F1 Score: 1.0\n",
      "Precision Score: 1.0\n",
      "Recall Score: 1.0\n",
      "---------------------------\n",
      "---- Participant id S7 ----\n",
      "[0.7755102 0.2244898] 0.2894736842105263\n",
      "Imbalanced\n",
      "Fitting 3 folds for each of 144 candidates, totalling 432 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   13.8s\n",
      "[Parallel(n_jobs=-1)]: Done 432 out of 432 | elapsed:   34.1s finished\n",
      "Best cv score: 1.0\n",
      "{'class_weight': None, 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "F1 Score: 0.888888888888889\n",
      "Precision Score: 0.8\n",
      "Recall Score: 1.0\n",
      "---------------------------\n",
      "---- Participant id S8 ----\n",
      "[0.7755102 0.2244898] 0.2894736842105263\n",
      "Imbalanced\n",
      "Fitting 3 folds for each of 144 candidates, totalling 432 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   13.9s\n",
      "[Parallel(n_jobs=-1)]: Done 432 out of 432 | elapsed:   33.9s finished\n",
      "Best cv score: 1.0\n",
      "{'class_weight': None, 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "F1 Score: 0.888888888888889\n",
      "Precision Score: 0.8\n",
      "Recall Score: 1.0\n",
      "---------------------------\n",
      "---- Participant id S9 ----\n",
      "[0.78 0.22] 0.28205128205128205\n",
      "Imbalanced\n",
      "Fitting 3 folds for each of 144 candidates, totalling 432 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   13.6s\n",
      "[Parallel(n_jobs=-1)]: Done 432 out of 432 | elapsed:   33.9s finished\n",
      "Best cv score: 1.0\n",
      "{'class_weight': None, 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "F1 Score: 1.0\n",
      "Precision Score: 1.0\n",
      "Recall Score: 1.0\n",
      "---------------------------\n",
      "------------------------------------\n",
      "Mean F1-score: 0.8085185185185186\n",
      "Mean precision score: 0.8192063492063493\n"
     ]
    }
   ],
   "source": [
    "f1_scores = []\n",
    "precision_scores = []\n",
    "test_groups = []\n",
    "for participant_id, data in person_specific_dataset.items():\n",
    "    print(f\"---- Participant id {participant_id} ----\")\n",
    "    person_specific_gt = person_specific_ground_truth[participant_id].flatten()\n",
    "    if len(list(set(person_specific_gt))) < 2: # If the data contains non-stress value, then continue\n",
    "        continue\n",
    "    test_groups.append(participant_id)\n",
    "    # person_specific_data, person_specific_gt = dataset_loader.divide_person_specific_data_into_intervals(data, person_specific_gt, num_samples=60, sampling_rate = 700)\n",
    "    # person_specific_statistic_features = extract_features(person_specific_data, sampling_rate = 700)\n",
    "    person_specific_data, person_specific_gt = dataset_loader.divide_person_specific_data_into_intervals(data, person_specific_gt, num_samples=60, sampling_rate = 4)\n",
    "    # person_specific_statistic_features = extract_features(person_specific_data, sampling_rate = 4)\n",
    "    # total_classes = len(person_specific_gt)\n",
    "    # class_percentage = np.array([num_items / total_classes for _, num_items in Counter(person_specific_gt).items()])\n",
    "    \n",
    "    # WRIST-WORN DEVICE ONLY\n",
    "    preprocessed_data, preprocessed_ground_truth, _ = preprocess_data(person_specific_data, person_specific_gt) \n",
    "    person_specific_statistic_features = extract_features(preprocessed_data, sampling_rate = 4)\n",
    "    total_classes = len(preprocessed_ground_truth)\n",
    "    class_percentage = np.array([num_items / total_classes for _, num_items in Counter(preprocessed_ground_truth).items()])\n",
    "\n",
    "    #  --------\n",
    "    balanced_degree_score = 1.0 * class_percentage.min() / class_percentage.max()\n",
    "    print(class_percentage, balanced_degree_score)\n",
    "    if balanced_degree_score < 0.5:\n",
    "        print(\"Imbalanced\")\n",
    "        # clf = Classifier(person_specific_statistic_features, person_specific_gt, balanced_weight = 'balanced', cross_validation = True, feature_labels = feature_labels)\n",
    "    else:\n",
    "        print(\"Balanced\")\n",
    "    # clf = Classifier(person_specific_statistic_features, person_specific_gt, cross_validation = True, feature_labels = feature_labels)\n",
    "    clf = Classifier(person_specific_statistic_features, preprocessed_ground_truth, cross_validation = True, feature_labels = feature_labels)\n",
    "    # f1score, precision = clf.logistic_regression()\n",
    "    f1score, precision = clf.random_forest_classifier()\n",
    "    # f1score, precision = clf.support_vector_machine()\n",
    "    # f1score, precision = clf.multilayer_perceptron()\n",
    "    # f1score, precision = clf.knn_classifier()\n",
    "    f1_scores.append(f1score)\n",
    "    precision_scores.append(precision)\n",
    "    print(\"---------------------------\")\n",
    "print(\"------------------------------------\")\n",
    "clf.dump_csv(test_groups, f1_scores, 'WESAD-Personal-RF.csv')\n",
    "mean_f1_score = np.mean(f1_scores)\n",
    "print(f\"Mean F1-score: {mean_f1_score}\")\n",
    "mean_precision_score = np.mean(precision_scores)\n",
    "print(f\"Mean precision score: {mean_precision_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}